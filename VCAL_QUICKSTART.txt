0) Install dfits or dfitspy (or any other fits header parser utility).
The latter can be installed with: pip install dfitspy

1) Download:
	- dataset of interest + raw calibs
	- FOR IFS: Download additionally ALL darks from the following morning manually (i.e. as another request). This is because the ESO algorithm only downloads the darks with same DIT as the OBJECT observations, but not the ones for flats and other calibs.
	=> Place all files in a folder named 'raw'

2) Uncompress all *.Z files

3) cd in the 'raw' directory, and use dfits or dfitspy to list relevant header values to infer the strategy used during the observation (optionally check some raw images):
	3a) Using dfits and fitsort:
		dfits SPHER*.fits |fitsort DET.ID DET.SEQ1.DIT INS1.FILT.NAME INS1.OPTI2.NAME DPR.TYPE INS4.COMB.ROT
	Pro tip: make an alias in your ~/.bash_profile e.g. by adding:
	alias dfits_irdifs='dfits SPHER*.fits |fitsort DET.ID DET.SEQ1.DIT INS.COMB.IFLT INS1.FILT.NAME INS1.OPTI2.NAME DPR.TYPE INS4.COMB.ROT'
	then simply call dfits_irdifs whenever needed in a given directory.

	3b) If you've installed dfitspy, open an ipython session in your 'raw' directory and run:
		import dfitspy
		listfiles = dfitspy.get_files(['all'])
		listkeys = ['HIERARCH ESO DET ID', 'HIERARCH ESO DET SEQ1 DIT', 'HIERARCH ESO INS1 FILT NAME', 'HIERARCH ESO INS1 OPTI2 NAME','HIERARCH ESO DPR TYPE' ,'HIERARCH ESO INS4 COMB ROT', 'HIERARCH ESO INS COMB IFLT']
		fitsort = dfitspy.dfitsort(listfiles, listkeys)
		dfitspy.dfitsort_view(fitsort)

4) If you plan to reduce IFS data, double-check the following calibration files are downloaded (it appears they are not systematically downloaded when ticking "download associated calibration files"):
- 'SPECPOS,LAMP'
- 'WAVE,LAMP'
If not present, you will need to download them separately from the ESO archive. Consider the calibration files that were obtained closest in time from the observations, in the same mode (INS2 COMB IFS header keyword) as the observations.

5) Make a copy of example JSON parameter files (in vcal_sphere/example) and place them into another directory (any directory but 'raw' you created above!). Then *adapt them* for your data:
	a) calibration (flat-fielding, sky subtraction, in addition for IFS: detector-level bad pixel correction, wavelength calibration, spectral cube creation)
	b) preprocessing (bad pixel correction, centering, bad frame removal, ADI cube creation, derotation angle calculation)
	c) post-processing (median-ADI; PCA-ADI full/ann; PCA-SADI/PCA-ASDI for IFS)
Description for each parameter is provided in the relevant MANUAL txt files.

6) Run VCAL:
	- cd in the directory with your adapted JSON parameter files, 
	- open an ipython/jupyter notebook session
	- import and run vcal functions, calling the relevant parameter files:
		a) For calibration, e.g.:
			from vcal.calib import calib
			calib('VCAL_params_calib.json')
		b) For pre-processing of IRDIS data, e.g.:
			from vcal.preproc import preproc_IRDIS
			preproc_IRDIS('VCAL_params_preproc_IRDIS.json', 'VCAL_params_calib.json')
		c) For post-processing of IRDIS data, e.g.:
		from vcal.postproc import postproc_IRDIS
			postproc_IRDIS('VCAL_params_postproc_IRDIS.json', 'VCAL_params_preproc_IRDIS.json', 'VCAL_params_calib.json')

Pro tip: instead of ipython/jupyter sessions, consider making a bash script for automated reductions.


Warning: Different observations may have different specific issues that may have not been predicted for handling by the pipeline. Inspect intermediate files for diagnostic, and iterate on the requested reduction parameters in the JSON files.
